\documentclass[a4paper, oneside]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{frontespizio}
\usepackage{float}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{scrextend}
\usepackage[margin=1.2in]{geometry}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{url}
\usepackage{verbatim}

\begin{document}
\selectlanguage{english}
\baselineskip 13pt

% ---- FRONTESPIZIO ----- 
\begin{frontespizio} 
 \Preambolo{\renewcommand{\frontpretitlefont}{\fontsize{15}{12}\scshape}}
\Istituzione {Universit√† di Pisa}
\Divisione {Scuola di Ingegneria}
\Corso [Laurea]{Artificial Intelligence and Data Engineering}
\Annoaccademico {2019--2020}
\Titolo {Cloud Computing project:\\The \textit{k}-means Clustering Algorithm in MapReduce}
%\Filigrana [height=4cm,before=0.28,after=1]{./images/stemma_unipi.png}
\Rientro {1cm}
\Candidato {Alice Nannini}
\Candidato {Fabio Malloggi}
\Candidato {Marco Parola}
\Candidato {Stefano Poleggi}
\Relatore {Dr. Nicola Tonellotto}
 \Punteggiatura {}
\end{frontespizio}

\clearpage

% ----- INDICE -----
\tableofcontents\thispagestyle{empty}
\clearpage

\title{KMEANS algorithm}\pagenumbering{arabic}

\section{Introduction}
This project presents the implementation of the Kmeans algorithm based on a MapReduce version, using both the Hadoop framework both the Spark framework.\\
The two implementations of the kmeans algorithm developed must be performed with the following inputs:
\begin{itemize}
\item Name of the input file containing the dataset
\item Number of centroids/clusters
\item Output directory
\item Number of total samples in the input dataset (the algorithm can be run assuming that you know this value)
\end{itemize}
\vspace{4mm}
The algorithm exit can occur due to two events:
\begin{itemize}
\item The maximum number of possible iteration has been reached 
\item The centroids calculated at i-th step and i+1-th step do not deviate beyond a certain threshold (Euclidean norm)
\end{itemize}

\section{Dataset}
The datasets for the final tests were generated with a python script, shown below and having the following format \textit{`dataset\_numPoints\_kClusters\_dimPoints'}.\\

\begin{verbatim}
import random

# inputs: n (records), k (clusters), d (dimensions)
numPoints = [1000,10000,100000]
kClusters = [7,13]
dimPoints = [3,7]


for n in numPoints:
    for k in kClusters:
        for d in dimPoints:
            # open a new file
            f = open("data/dataset_"+str(n)+"_"+str(k)+"_"+str(d)+".txt", "a")
            
            # compute the interval for creating the clusters
            interval = round(n/(2*k))
            count = 0
            print("dataset_"+str(n)+"_"+str(k)+"_"+str(d)+"; int: "+str(interval))
            
            # compute each point
            for i in range(n):
                if( (i%interval)==0 and i!=0):
                    count = count + 2
                
                x = ""
                for j in range(d):
                    x = x + str( interval*count + random.random()*interval )
                    x = x + " "
                x = x + "\n"
                # write the new point coordinates in the file
                f.write(x)
            
            f.close()
\end{verbatim}

\vspace{4mm}
List of files generated from the previous code:
\begin{itemize}
\item dataset\_100000\_13\_3.txt
\item dataset\_100000\_13\_7.txt
\item dataset\_100000\_7\_3.txt
\item dataset\_100000\_7\_7.txt
\item dataset\_10000\_13\_3.txt
\item dataset\_10000\_13\_7.txt
\item dataset\_10000\_7\_3.txt
\item dataset\_10000\_7\_7.txt
\item dataset\_1000\_13\_3.txt
\item dataset\_1000\_13\_7.txt
\item dataset\_1000\_7\_3.txt
\item dataset\_1000\_7\_7.txt 
\end{itemize}

\section{MapReduce pseudo-code}
\begin{verbatim}
class MAPPER
  method MAP(sample_id id, sample_list l)
    for all sample s in sample_list l do
      dist = MAX_VALUE
      for all center c in cluster_centers cc do
        newDist <- computeDistance(s, c)
        if newDist < dist
          dist <- newDist
          clusterIndex = cc.index
      EMIT(index clusterIndex, sample s)


class REDUCER
  method REDUCE(index clusterIndex, samples [s1, s2,...])
    count <- 0
    center <- cluster_centers[clusterIndex]
    for all sample s in samples [s1, s2,...] do
      count <- count + 1
      for i in [0:size(s)] do
        newCenter[i] <- newCenter[i] + s[i]
    for i in [0:size(newCenter)] do
      newCenter[i] <- newCenter[i] / count
    EMIT(index clusterIndex, sample newCenter)
\end{verbatim}

\section{Hadoop Implementation}
\section{Spark Implementation}
\section{Tests and Results}



\end{document}
